
import re
import streamlit as st
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from stream_answer import stream_ans
embedding_model="./all-MiniLM-L6-v2"
vecterdb_path='./question_db'

def init_chat_history():
    st.write("è¯·è¾“å…¥é—®é¢˜å’Œç­”æ¡ˆï¼Œæˆ‘å°†å°è¯•ç”Ÿæˆç›¸ä¼¼çš„é¢˜ç›®")
    #æå‰åŠ è½½çŸ¥è¯†åº“,å¤§æ¦‚ä¸¤åˆ†é’Ÿ
    if "question_db" not in st.session_state:
        st.session_state.question_db = None
# æç¤ºè¯å‚è€ƒ  https://github.com/codelion/optillm/blob/main/optillm/cot_reflection.py
def cot(system_prompt, user_prompt):
    cot_prompt = """
    {}
    ä½ æ˜¯æ•°å­¦å‘½åˆ¶é¢˜ç›®å°èƒ½æ‰‹
    ä½ æ“…é•¿ä½¿ç”¨æ€ç»´é“¾ï¼ˆChain of Thoughtï¼Œç®€ç§°CoTï¼‰æ–¹æ³•å¹¶ç»“åˆåæ€æ¥å®Œæˆæ•°å­¦é¢˜ç›®å‘½åˆ¶ã€‚éµå¾ªä»¥ä¸‹æ­¥éª¤:

    1. åœ¨<analysis>æ ‡ç­¾å†…åˆ†æçŸ¥è¯†ç‚¹ï¼Œä»¥åŠç­”æ¡ˆçš„æ€æƒ³æ–¹æ³•
    2. åœ¨ <thinking> æ ¹æ®åˆ†æå‡ºçš„çŸ¥è¯†ç‚¹ï¼Œä»¥åŠæ€æƒ³æ–¹æ³•å‘½åˆ¶é¢˜ç›®ã€‚
    3. åœ¨ <reflection> æ ‡ç­¾å†…åæ€ä½ <thinking>å†…å®¹ï¼Œæ£€æŸ¥å…¶ä¸­æ˜¯å¦å­˜åœ¨é”™è¯¯æˆ–å¯ä»¥æ”¹è¿›çš„åœ°æ–¹ã€‚
    4. æ ¹æ®ä½ çš„åæ€è¿›è¡Œå¿…è¦çš„è°ƒæ•´
    5. åœ¨ <output> æ ‡ç­¾å†…æä¾›ä½ æœ€ç»ˆç®€æ´çš„ç­”æ¡ˆã€‚

    è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿›è¡Œå›ç­”ï¼š
    <analysis>
    [ä½ å¯¹é¢˜ç›®çš„çŸ¥è¯†ç‚¹åˆ†æï¼Œåˆ†æ]
    </analysis>
    <thinking>
    [ä½ çš„å‘½é¢˜è¿‡ç¨‹å†™åœ¨è¿™é‡Œã€‚è¿™æ˜¯ä½ çš„å‘½åˆ¶é¢˜ç›®çš„å°è¯•ï¼Œä¸æ˜¯æœ€ç»ˆé¢˜ç›®ã€‚]
    <reflection>
    [ä½ å¯¹ä½ å‘½åˆ¶çš„é¢˜ç›®è¿›è¡Œåæ€ï¼Œæ£€æŸ¥å…¶ä¸­æ˜¯å¦å­˜åœ¨é”™è¯¯æˆ–å¯ä»¥æ”¹è¿›çš„åœ°æ–¹]
    </reflection>
    [æ ¹æ®ä½ å‘½åˆ¶çš„é¢˜ç›®åšçš„ä»»ä½•è°ƒæ•´]
    </thinking>
    <output>
    [ä½ å‘½åˆ¶çš„æœ€ç»ˆé¢˜ç›®]
    </output>
    """.format(system_prompt)
    history=[
            {"role": "system", "content": cot_prompt},
            {"role": "user", "content": user_prompt}
        ]
    response = stream_ans(history)
    return response
   


def main():
        init_chat_history()
        if prompt := st.chat_input("è¯·è¾“å…¥é—®é¢˜å’Œç­”æ¡ˆ"):
            with st.chat_message("user", avatar='ğŸ§‘'):
                st.markdown(prompt)
            search_results_string = ""
            st.write("æŸ¥è¯¢çŸ¥è¯†åº“â€¦â€¦")
            if  st.session_state.question_db:
                search_results = st.session_state.question_db.similarity_search(prompt, k=2)#resultæ˜¯document
            else:
                st.write("é¦–æ¬¡è®¿é—®éœ€åŠ è½½çŸ¥è¯†åº“...è¯·ç¨å")
                embeddings = HuggingFaceEmbeddings(model_name=embedding_model)
                vectorstore = FAISS.load_local(folder_path=vecterdb_path, embeddings=embeddings, allow_dangerous_deserialization=True)
                st.session_state.question_db = vectorstore
                st.write("çŸ¥è¯†åº“åŠ è½½å®Œæˆ")
                search_results = st.session_state.question_db.similarity_search(prompt, k=2)
            for result in search_results:
                search_results_string += result.page_content + "\n"
            with st.expander("æŸ¥è¯¢ç»“æœ"):
                st.write(search_results)
            response=cot(search_results_string,prompt+'æ— éœ€è§£ç­”é¢˜ç›®ï¼Œæ ¹æ®å·²æœ‰ä¿¡æ¯å‘½åˆ¶ç±»ä¼¼çš„é¢˜ç›®')
             # è¾“å‡ºæ€è€ƒç»“æœ
            if response:
                match = re.search(r"<output>(.*?)(?:</output>|$)", response, re.DOTALL)
                result= match.group(1).strip() if match else response
                with st.chat_message("assistant", avatar='ğŸ¤–'):
                    st.markdown(result)
                return result
            else: return ""
            
            
if __name__ == "__main__":
    main()